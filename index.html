<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dota - Tianai Dong</title>

  <meta name="author" content="Dota">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h1>Dota - Tianai Dong</h1>
              </p>
             <p>
Hi! I am a third-year PhD student, affiliated with <a href="https://www.mpi.nl/department/multimodal-language-department/23" target="_blank"> Multimodal Language Department </a> at the Max Planck Institute for Psycholinguistics, and <a href="https://www.predictivebrainlab.com/" target="_blank"> Predictive Brain Lab</a> at the Donders Institute (Centre for Cognitive Neuroimaging). I am co-advised by <a href="https://www.predictivebrainlab.com/people/floris-de-lange/" target="_blank"> Floris de Lange </a>, <a href="https://www.predictivebrainlab.com/people/lea-maria-schmitt/" target="_blank"> Lea-Maria Schmitt
 </a>, <a href="http://www.stefanfrank.info/" target="_blank"> Stefan Frank</a>, and  <a href="https://scholar.google.com/citations?user=FVaxbrsAAAAJ&hl=en" target="_blank"> Paula Rubio-Fern√°ndez </a>. I also work closely with <a href="https://mtoneva.com/" target="_blank"> Mariya Toneva </a> at the Max Planck Institute for Software Systems. I am funded by an <a href="https://www.mpi.nl/department/imprs-graduate-school/13" target="_blank"> IMPRS fellowship</a>.
              <p>
   I study how humans acquire, mentally represent, and generate predictions about language through our rich multimodal experiences. My approach combines computational methods with insights from neuroscience, linguistics, and psychology, with the dual goals of understanding the human mind and advancing artificial intelligence.


<br> <p>  If you want to discuss any academia-related topics, please feel free to reach out to me :) 
<p> <a href="mailto:tianai DOT dong AT mpi.nl"> Email // </a> <a href="https://scholar.google.com/citations?user=AuKjLM0AAAAJ&hl=en"> Google Scholar //</a></a> <a href="https://twitter.com/DotaDong"> Twitter </a><br>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="DSCF6546.jpeg"><img style="width:90%;max-width:90%" alt="profile photo" src="DSCF6546.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Papers</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='merlotbrain.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2311.07766.pdf">
                <papertitle> Multimodal Video Transformers Partially Align with Multimodal Grounding and Compositionality in the Brain</papertitle>
              </a>
              <br>
              <strong>Dota Tianai Dong</strong>,
              Mariya Toneva
              <br>
              <em><a href="https://openreview.net/forum?id=p-vL3rmYoqh">ICLR-MRL</a></em>, 2023;
              <em><a href="https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3527860">CCN</a></em>, 2023;
              <em><a href="https://arxiv.org/pdf/2311.07766.pdf">Preprint</a></em>, 2024
              <br>
              <p> We propose to probe a pre-trained multimodal video transformer model, guided by insights from neuroscientific evidence on multimodal information processing in the human brain. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='discogem.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://pure.mpg.de/rest/items/item_3488237/component/file_3488238/content">
                <papertitle>Discogem: A crowdsourced corpus of genre-mixed implicit discourse relations</papertitle>
              </a>
              <br>
              Merel Scholman
              <strong>Dota Tianai Dong</strong>,
              Frances Yung,
              Vera Demberg
              <br>
              <em><a href="https://pure.mpg.de/rest/items/item_3488237/component/file_3488238/content">LREC</a></em>, 2022;
              <br>
              <p>We present DiscoGeM, a crowdsourced corpus of 6,505 implicit discourse relations from three genres: political speech,
literature, and encyclopedic text.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='comparedis.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://aclanthology.org/2021.codi-main.9.pdf">
                <papertitle>Comparison of methods for explicit discourse connective identification across various domains</papertitle>
              </a>
              <br>
              Merel Scholman
              <strong>Dota Tianai Dong</strong>,
              Frances Yung,
              Vera Demberg
              <br>
              <em><a href="https://aclanthology.org/2021.codi-main.9.pdf">CODI</a></em>, 2021;
              <br>
              <p>We assess the performance on explicit
connective identification of four parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang and Lan, 2015; DisSent, Nie et al., 2019; and Discopy, Knaebel and Stede, 2020), along with a simple heuristic. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='orange.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://aclanthology.org/2021.splurobonlp-1.3/">
                <papertitle>Visually grounded follow-up questions: A dataset of spatial questions which require dialogue history</papertitle>
              </a>
              <br>
              <strong>Dota Tianai Dong</strong>,
              Alberto Testoni, 
              Luciana Benotti, 
              Raffaella Bernardi
              <br>
              <em><a href="https://aclanthology.org/2021.splurobonlp-1.3/">Splurobonlp</a></em>, 2021;
              <br>
              <p> We define and evaluate a methodology for extracting history-dependent spatial questions from visual dialogues.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Others</heading>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr >
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src="nest.png" height="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://drive.google.com/file/d/1p8VNFE_wBZIHY9m9v4yxhzRV77gUg1kn/view?usp=sharing">
              <papertitle>Analyses of Multiple Discourse Relations within a Chinese Sentence</papertitle>
            </a>
            <br>
            <strong>Dota Tianai Dong</strong>, Bonnie Webber, Jennifer Spenader
            <br>
            <a href="https://drive.google.com/file/d/1p8VNFE_wBZIHY9m9v4yxhzRV77gUg1kn/view?usp=sharing">Bachelor Thesis</a>
            <p></p>
          </td>
        </tr>


        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
          <tbody><tr>
            <td>
              <br>
              <p align="center">
                <font size="2">
                  Template from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
                </font>
              </p>
            </td></tr>
          </tbody>
        </table>

</body>

</html>
