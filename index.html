<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dota - Tianai Dong</title>

  <meta name="author" content="Dota">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h1>Dota - Tianai Dong</h1>
              </p>
             <p>
Hi! I am a third-year PhD student, affiliated with <a href="https://www.mpi.nl/department/multimodal-language-department/23" target="_blank"> Multimodal Language Department </a> at the Max Planck Institute for Psycholinguistics, and <a href="https://www.predictivebrainlab.com/" target="_blank"> Predictive Brain Lab</a> at the Donders Institute (Centre for Cognitive Neuroimaging). I am co-advised by <a href="https://www.predictivebrainlab.com/people/floris-de-lange/" target="_blank"> Floris de Lange </a>, <a href="https://www.predictivebrainlab.com/people/lea-maria-schmitt/" target="_blank"> Lea-Maria Schmitt
 </a>, <a href="http://www.stefanfrank.info/" target="_blank"> Stefan Frank</a>, and  <a href="https://scholar.google.com/citations?user=FVaxbrsAAAAJ&hl=en" target="_blank"> Paula Rubio-FernÃ¡ndez </a>. I am funded by an <a href="https://www.mpi.nl/department/imprs-graduate-school/13" target="_blank"> IMPRS fellowship</a>.
              <p>
   I study how humans acquire, mentally represent, and generate predictions about language through our rich multimodal experiences. My approach combines computational methods with insights from neuroscience, linguistics, and psychology, with the dual goals of understanding the human mind and advancing artificial intelligence.


<br> <p>  If you want to discuss any academia-related topics, please feel free to reach out to me :) 
<p> <a href="mailto:tianai DOT dong AT mpi.nl"> Email // </a> <a href="https://scholar.google.com/citations?user=AuKjLM0AAAAJ&hl=en"> Google Scholar //</a></a> <a href="https://bsky.app/profile/dotadotadota.bsky.social"> BlueSky </a><br>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="me.jpeg"><img style="width:90%;max-width:90%" alt="profile photo" src="me.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <div style="display:flex;flex-direction:column;gap:25px;">
                <div style="display:flex;align-items:center;background-color:#f8f8f8;border-radius:8px;padding:15px;box-shadow:0 2px 4px rgba(0,0,0,0.1);">
                  <div style="min-width:80px;text-align:center;margin-right:15px;">
                    <div style="background-color:#4a86e8;color:white;font-weight:bold;padding:5px;border-radius:4px;">2025 May</div>
                  </div>
                  <div>
                    <p style="margin:0;font-weight:500;"> ðŸ’¡ <a href="https://arxiv.org/abs/2506.00065" target="_blank" style="text-decoration:none;"><span style="color:#4a86e8;font-weight:bold;">You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models</span></a> is presented as a spotlight talk at PragLM@Colm.</p>
                  </div>
                </div>
                
                <div style="display:flex;align-items:center;background-color:#f8f8f8;border-radius:8px;padding:15px;box-shadow:0 2px 4px rgba(0,0,0,0.1);">
                  <div style="min-width:80px;text-align:center;margin-right:15px;">
                    <div style="background-color:#4a86e8;color:white;font-weight:bold;padding:5px;border-radius:4px;">2025 May</div>
                  </div>
                  <div>
                    <p style="margin:0;font-weight:500;">I gave a talk at <a href="https://webapps.unitn.it/cimec/" target="_blank" style="text-decoration:none;"><span style="color:#4a86e8;font-weight:bold;">CIMEC computational linguistics</span></a>, on "Grounding <strong>Language (and Language Models)</strong> by <strong>Seeing</strong>, <strong>Hearing</strong>, and <strong>Interacting</strong>"</p>
                  </div>
                </div>
              </div>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Papers</heading>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='reference_MLM.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2506.00065">
                <papertitle>You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models</papertitle>
              </a>
              <br>
              <strong>Dota Tianai Dong</strong> (co-first),
              Yifan Luo (co-first),
              Po-Ya Angela Wang,
              Asli Ozyurek,
              Paula Rubio-Fernandez
              <br>
              <em><a href="https://sites.google.com/berkeley.edu/praglm/">SpotlightðŸ’¡PragLM@Colm</a></em>, 2025;
              <em><a href="https://arxiv.org/abs/2506.00065">Preprint</a></em>, 2025
              <br>
              <p>We evaluated seven MLMs against humans on three word classes: vocabulary words, possessive pronouns, and demonstrative pronouns. We observed a consistent difficulty hierarchy shared by both humans and models, but a clear performance gap remains: while MLMs approach human-level performance on vocabulary tasks, they show substantial deficits with possessive and demonstrative pronouns.</p>
            </td>
          </tr>
 

            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='merlotbrain.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2311.07766.pdf">
                <papertitle> Multimodal Video Transformers Partially Align with Multimodal Grounding and Compositionality in the Brain</papertitle>
              </a>
              <br>
              <strong>Dota Tianai Dong</strong>,
              Mariya Toneva
              <br>
              <em><a href="https://openreview.net/forum?id=p-vL3rmYoqh">ICLR-MRL</a></em>, 2023;
              <em><a href="https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3527860">CCN</a></em>, 2023;
              <em><a href="https://arxiv.org/pdf/2311.07766.pdf">Preprint</a></em>, 2024
              <br>
              <p> We propose to probe a pre-trained multimodal video transformer model, guided by insights from neuroscientific evidence on multimodal information processing in the human brain. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='discogem.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://pure.mpg.de/rest/items/item_3488237/component/file_3488238/content">
                <papertitle>Discogem: A crowdsourced corpus of genre-mixed implicit discourse relations</papertitle>
              </a>
              <br>
              Merel Scholman
              <strong>Dota Tianai Dong</strong>,
              Frances Yung,
              Vera Demberg
              <br>
              <em><a href="https://pure.mpg.de/rest/items/item_3488237/component/file_3488238/content">LREC</a></em>, 2022;
              <br>
              <p>We present DiscoGeM, a crowdsourced corpus of 6,505 implicit discourse relations from three genres: political speech,
literature, and encyclopedic text.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='comparedis.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://aclanthology.org/2021.codi-main.9.pdf">
                <papertitle>Comparison of methods for explicit discourse connective identification across various domains</papertitle>
              </a>
              <br>
              Merel Scholman
              <strong>Dota Tianai Dong</strong>,
              Frances Yung,
              Vera Demberg
              <br>
              <em><a href="https://aclanthology.org/2021.codi-main.9.pdf">CODI</a></em>, 2021;
              <br>
              <p>We assess the performance on explicit
connective identification of four parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang and Lan, 2015; DisSent, Nie et al., 2019; and Discopy, Knaebel and Stede, 2020), along with a simple heuristic. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='orange.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://aclanthology.org/2021.splurobonlp-1.3/">
                <papertitle>Visually grounded follow-up questions: A dataset of spatial questions which require dialogue history</papertitle>
              </a>
              <br>
              <strong>Dota Tianai Dong</strong>,
              Alberto Testoni, 
              Luciana Benotti, 
              Raffaella Bernardi
              <br>
              <em><a href="https://aclanthology.org/2021.splurobonlp-1.3/">Splurobonlp</a></em>, 2021;
              <br>
              <p> We define and evaluate a methodology for extracting history-dependent spatial questions from visual dialogues.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Service</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <ul style="line-height:1.8;">
              <li><strong>2026:</strong> DEI Chair, <a href="https://2025.ccneuro.org/" target="_blank">CCN</a></li>
              <li><strong>2025:</strong> Co-organizer, <a href="https://donders-neuro-ai-focus-group.github.io/" target="_blank">Donders Neuro-AI Focus Group</a></li>
              <li><strong>2025:</strong> Co-organizer, <a href="https://2025.ccneuro.org/community-event-representational-alignment/" target="_blank">Re^3-Align Collaborative Hackathon @ CCN</a></li>
              <li><strong>2025:</strong> DEI Committee Member, <a href="https://2025.ccneuro.org/" target="_blank">CCN</a></li>
              <li><strong>2025:</strong> Co-organizer, <a href="https://representational-alignment.github.io/2025/" target="_blank">Re^2-Align Workshop @ ICLR</a></li>
             </ul>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Others</heading>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr >
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src="nest.png" height="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://drive.google.com/file/d/1p8VNFE_wBZIHY9m9v4yxhzRV77gUg1kn/view?usp=sharing">
              <papertitle>Analyses of Multiple Discourse Relations within a Chinese Sentence</papertitle>
            </a>
            <br>
            <strong>Dota Tianai Dong</strong>, Bonnie Webber, Jennifer Spenader
            <br>
            <a href="https://drive.google.com/file/d/1p8VNFE_wBZIHY9m9v4yxhzRV77gUg1kn/view?usp=sharing">Bachelor Thesis</a>
            <p></p>
          </td>
        </tr>


        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
          <tbody><tr>
            <td>
              <br>
              <p align="center">
                <font size="2">
                  Template from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
                </font>
              </p>
            </td></tr>
          </tbody>
        </table>

</body>

</html>
